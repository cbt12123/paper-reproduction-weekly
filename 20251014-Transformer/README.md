# 论文复现：Attention Is All You Need（Transformer）
- **论文链接**：https://arxiv.org/abs/1706.03762
- **复现日期**：2025.10.13-2025.10.20
- **运行环境**：Python 3.9 + PyTorch 2.0 + CUDA 11.7

## 1. 论文核心梳理
### 核心问题
传统 RNN/CNN 处理序列数据时，存在“并行性差”“长距离依赖捕捉弱”的问题。

### 核心方案
提出 **Transformer 架构**，完全基于自注意力（Self-Attention）机制，替代循环结构，实现并行计算，同时强化长距离依赖捕捉。

### 关键模块
- 自注意力（Self-Attention）：计算序列内每个token与其他token的关联权重；
- 多头注意力（Multi-Head Attention）：并行执行多个自注意力，捕捉不同维度的关联；
- 位置编码（Positional Encoding）：为无顺序的注意力机制注入序列位置信息。

## 2. 复现细节
### 依赖说明
见 `requirements.txt`，核心依赖：`torch==2.0.1`、`numpy==1.24.3`、`matplotlib==3.7.1`（用于画图）。

### 代码结构
- `transformer.ipynb`：核心代码，按“模块拆分”编写（位置编码→自注意力→多头注意力→编码器/解码器→完整模型）；
- `data/`：使用 IWSLT2016 德英翻译数据集，通过 `torchtext` 自动下载（代码内有注释）；
- `results/`：训练过程的 loss 曲线（`loss_curve.png`）、测试集翻译示例（`translation_example.txt`）。

### 关键问题与解决
1. **问题**：自注意力计算时，softmax 前数值过大导致梯度消失；  
   **解决**：添加“缩放因子”（√dk，dk为词向量维度），缓解数值问题。
2. **问题**：解码器的“掩码注意力”（Masked Multi-Head Attention）需屏蔽未来token；  
   **解决**：生成下三角掩码矩阵，将未来位置的权重设为 -1e9，softmax后趋近于0。

## 3. 复现结果对比
| 指标         | 论文结果 | 我的复现结果 | 差异原因               |
|--------------|----------|--------------|------------------------|
| BLEU（德英） | 28.4     | 27.9         | 训练轮次减少（论文100轮，我50轮） |
| 训练速度     | -        | 比RNN快30%   | 符合论文“并行计算”优势 |

## 4. 总结
通过复现，明确了 Transformer 并行性的核心来源（无循环依赖），也理解了“位置编码”的必要性（无位置信息时，模型无法区分序列顺序）。后续可优化方向：加入预训练机制（如 BERT 的基础），提升泛化能力。